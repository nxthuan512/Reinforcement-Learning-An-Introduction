{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 1.5: Tic-Tac-Toe",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxthuan512/Reinforcement-Learning-An-Introduction/blob/master/Chapter_1_5_Tic_Tac_Toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "WmE5MZq3_fsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ]
    },
    {
      "metadata": {
        "id": "8QI95PEi_nGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook shows the codes of the Tic-tac-toe game described in the **Reinforcement Learning - An Introduction** *(Sutton and Barton March 2018)* textbook. The following codes are adjusted based on the **Artificial Intelligence: Reinforcement Learning in Python** course *(Lazy Programmer)*. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**References**:\n",
        "1.   [Reinforcement Learning - An Introduction](https://github.com/t0nberryking/Reinforcement-Learning) textbook\n",
        "2.   [Artificial Intelligence: Reinforcement Learning in Python](https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python) course"
      ]
    },
    {
      "metadata": {
        "id": "Tkx8lqm3_1Vz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Tic-Tac-Toe: Epsilon-Greedy \n",
        "**Ref. 1**, Chapter 1.5, pp. 28/548 \\\\\n",
        "**Ref. 2**, Section 4"
      ]
    },
    {
      "metadata": {
        "id": "pldqZmFuWMQP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
        "# https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python\n",
        "# Simple reinforcement learning algorithm for learning tic-tac-toe\n",
        "# Use the update rule: V(s) = V(s) + alpha*(V(s') - V(s))\n",
        "# Use the epsilon-greedy policy:\n",
        "#   action|s = argmax[over all actions possible from state s]{ V(s) }  if rand > epsilon\n",
        "#   action|s = select random action from possible actions from state s if rand < epsilon\n",
        "#\n",
        "#\n",
        "# INTERESTING THINGS TO TRY:\n",
        "#\n",
        "# Currently, both agents use the same learning strategy while they play against each other.\n",
        "# What if they have different learning rates?\n",
        "# What if they have different epsilons? (probability of exploring)\n",
        "#   Who will converge faster?\n",
        "# What if one agent doesn't learn at all?\n",
        "#   Poses an interesting philosophical question: If there's no one around to challenge you,\n",
        "#   can you reach your maximum potential?\n",
        "from __future__ import print_function, division\n",
        "from builtins import range, input\n",
        "# Note: you may need to update your version of future\n",
        "# sudo pip install -U future\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\t\n",
        "LENGTH = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJ6LvbaMnyEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TTT Environment\n",
        "# ============================================================\n",
        "class Environment:\n",
        "  # -----------------------------\n",
        "  def __init__(self):\n",
        "    self.board = np.zeros((LENGTH, LENGTH))\n",
        "    self.x = -1 # represents an x on the board, player 1\n",
        "    self.o = 1 # represents an o on the board, player 2\n",
        "    self.winner = None\n",
        "    self.ended = False\n",
        "    self.num_states = 3**(LENGTH*LENGTH)\n",
        "    \n",
        "  # -----------------------------\n",
        "  def is_empty(self, i, j):\n",
        "    return self.board[i,j] == 0\n",
        "  \n",
        "  # -----------------------------\n",
        "  def reward(self, sym):\n",
        "    # no reward until game is over\n",
        "    if not self.game_over():\n",
        "      return 0\n",
        "\n",
        "    # if we get here, game is over\n",
        "    # sym will be self.x or self.o\n",
        "    return 1 if self.winner == sym else 0\n",
        "\n",
        "  # -----------------------------\n",
        "  def get_state(self):\n",
        "    # returns the current state, represented as an int\n",
        "    # from 0...|S|-1, where S = set of all possible states\n",
        "    # |S| = 3^(BOARD SIZE), since each cell can have 3 possible values - empty, x, o\n",
        "    # some states are not possible, e.g. all cells are x, but we ignore that detail\n",
        "    # this is like finding the integer represented by a base-3 number\n",
        "    k = 0\n",
        "    h = 0\n",
        "    for i in range(LENGTH):\n",
        "      for j in range(LENGTH):\n",
        "        if self.board[i,j] == 0:\n",
        "          v = 0\n",
        "        elif self.board[i,j] == self.x:\n",
        "          v = 1\n",
        "        elif self.board[i,j] == self.o:\n",
        "          v = 2\n",
        "        h += (3**k) * v\n",
        "        k += 1\n",
        "    return h\n",
        "\n",
        "  # -----------------------------\n",
        "  def game_over(self, force_recalculate=False):\n",
        "    # returns true if game over (a player has won or it's a draw)\n",
        "    # otherwise returns false\n",
        "    # also sets 'winner' instance variable and 'ended' instance variable\n",
        "    if not force_recalculate and self.ended:\n",
        "      return self.ended\n",
        "    \n",
        "    # check rows\n",
        "    for i in range(LENGTH):\n",
        "      for player in (self.x, self.o):\n",
        "        if self.board[i].sum() == player*LENGTH:\n",
        "          self.winner = player\n",
        "          self.ended = True\n",
        "          return True\n",
        "\n",
        "    # check columns\n",
        "    for j in range(LENGTH):\n",
        "      for player in (self.x, self.o):\n",
        "        if self.board[:,j].sum() == player*LENGTH:\n",
        "          self.winner = player\n",
        "          self.ended = True\n",
        "          return True\n",
        "\n",
        "    # check diagonals\n",
        "    for player in (self.x, self.o):\n",
        "      # top-left -> bottom-right diagonal\n",
        "      if self.board.trace() == player*LENGTH:\n",
        "        self.winner = player\n",
        "        self.ended = True\n",
        "        return True\n",
        "      # top-right -> bottom-left diagonal\n",
        "      if np.fliplr(self.board).trace() == player*LENGTH:\n",
        "        self.winner = player\n",
        "        self.ended = True\n",
        "        return True\n",
        "\n",
        "    # check if draw\n",
        "    if np.all((self.board == 0) == False):\n",
        "      # winner stays None\n",
        "      self.winner = None\n",
        "      self.ended = True\n",
        "      return True\n",
        "\n",
        "    # game is not over\n",
        "    self.winner = None\n",
        "    return False\n",
        "\n",
        "  # -----------------------------\n",
        "  def is_draw(self):\n",
        "    return self.ended and self.winner is None\n",
        "\n",
        "  # -----------------------------\n",
        "  # Example board\n",
        "  # -------------\n",
        "  # | x |   |   |\n",
        "  # -------------\n",
        "  # |   |   |   |\n",
        "  # -------------\n",
        "  # |   |   | o |\n",
        "  # -------------\n",
        "  def draw_board(self):\n",
        "    for i in range(LENGTH):\n",
        "      print(\"-------------\")\n",
        "      for j in range(LENGTH):\n",
        "        print(\"  \", end=\"\")\n",
        "        if self.board[i,j] == self.x:\n",
        "          print(\"x \", end=\"\")\n",
        "        elif self.board[i,j] == self.o:\n",
        "          print(\"o \", end=\"\")\n",
        "        else:\n",
        "          print(\"  \", end=\"\")\n",
        "      print(\"\")\n",
        "    print(\"-------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8obGsSWqCRyb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Vahjya0bV0bj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TTT Agent\n",
        "# ============================================================\n",
        "class Agent:\n",
        "  # -----------------------------\n",
        "  def __init__(self, eps=0.1, alpha=0.5):\n",
        "    self.eps = eps # probability of choosing random action instead of greedy\n",
        "    self.alpha = alpha # learning rate\n",
        "    self.verbose = False\n",
        "    self.state_history = []\n",
        "  \n",
        "  # -----------------------------\n",
        "  def setV(self, V):\n",
        "    self.V = V\n",
        "\n",
        "  # -----------------------------\n",
        "  def set_symbol(self, sym):\n",
        "    self.sym = sym\n",
        "\n",
        "  # -----------------------------\n",
        "  def set_verbose(self, v):\n",
        "    # if true, will print values for each position on the board\n",
        "    self.verbose = v\n",
        "\n",
        "  # -----------------------------\n",
        "  def reset_history(self):\n",
        "    self.state_history = []\n",
        "\n",
        "  # -----------------------------\n",
        "  def take_action(self, env):\n",
        "    r = np.random.rand()\n",
        "    best_state = None\n",
        "    flag = 0\n",
        "    \n",
        "    # choose an action based on epsilon-greedy strategy\n",
        "    if r < self.eps:\n",
        "      possible_moves = []\n",
        "      for i in range(LENGTH):\n",
        "        for j in range(LENGTH):\n",
        "          if env.is_empty(i, j):\n",
        "            possible_moves.append((i, j))\n",
        "      idx = np.random.choice(len(possible_moves))\n",
        "      next_move = possible_moves[idx] \n",
        "      flag = 1\n",
        "    \n",
        "    # choose the best action based on current values of states\n",
        "    # loop through all possible moves, get their values\n",
        "    # keep track of the best value\n",
        "    else:\n",
        "      pos2value = {} # for debugging\n",
        "      next_move = None\n",
        "      best_value = -1\n",
        "      for i in range(LENGTH):\n",
        "        for j in range(LENGTH):\n",
        "          if env.is_empty(i, j):\n",
        "            # what is the state if we made this move?\n",
        "            env.board[i,j] = self.sym\n",
        "            state = env.get_state()\n",
        "            env.board[i,j] = 0 # don't forget to change it back!\n",
        "            pos2value[(i,j)] = self.V[state]\n",
        "            if self.V[state] > best_value:\n",
        "              best_value = self.V[state]\n",
        "              best_state = state\n",
        "              next_move = (i, j)\n",
        "        \n",
        "    # make the move\n",
        "    env.board[next_move[0], next_move[1]] = self.sym\n",
        "    \n",
        "    # if verbose, draw the board w/ the values\n",
        "    if self.verbose:\n",
        "      if self.sym == env.x and flag == 0:\n",
        "        print(\"X: Taking a greedy action\")\n",
        "      elif self.sym == env.x and flag == 1:\n",
        "        print(\"X: Taking a random action\")\n",
        "      elif self.sym == env.o and flag == 0:\n",
        "        print(\"O: Taking a greedy action\")\n",
        "      elif self.sym == env.o and flag == 1:\n",
        "        print(\"O: Taking a random action\")\n",
        "        \n",
        "      for i in range(LENGTH):\n",
        "        print(\"------------------\")\n",
        "        for j in range(LENGTH):\n",
        "          if env.is_empty(i, j):\n",
        "            # print the value\n",
        "            state = env.get_state()\n",
        "            print(\" %.2f|\" % self.V[state], end=\"\")\n",
        "          else:\n",
        "            print(\"  \", end=\"\")\n",
        "            if env.board[i,j] == env.x:\n",
        "              print(\"x  |\", end=\"\")\n",
        "            elif env.board[i,j] == env.o:\n",
        "              print(\"o  |\", end=\"\")\n",
        "            else:\n",
        "              print(\"   |\", end=\"\")\n",
        "        print(\"\")\n",
        "      print(\"------------------\")\n",
        "\n",
        "  # -----------------------------\n",
        "  def update_state_history(self, s):\n",
        "    # cannot put this in take_action, because take_action only happens\n",
        "    # once every other iteration for each player\n",
        "    # state history needs to be updated every iteration\n",
        "    # s = env.get_state() # don't want to do this twice so pass it in\n",
        "    self.state_history.append(s)\n",
        "\n",
        "  # -----------------------------\n",
        "  def update(self, env):\n",
        "    # we want to BACKTRACK over the states, so that:\n",
        "    # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state))\n",
        "    # where V(next_state) = reward if it's the most current state\n",
        "    #\n",
        "    # NOTE: we ONLY do this at the end of an episode\n",
        "    # not so for all the algorithms we will study\n",
        "    reward = env.reward(self.sym)\n",
        "    target = reward\n",
        "    for prev in reversed(self.state_history):\n",
        "      value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
        "      self.V[prev] = value\n",
        "      target = value\n",
        "    self.reset_history()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ta-NLPo5WBfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TTT Human\n",
        "# ============================================================\n",
        "class Human:\n",
        "  # -----------------------------\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # -----------------------------\n",
        "  def set_symbol(self, sym):\n",
        "    self.sym = sym\n",
        "\n",
        "  # -----------------------------\n",
        "  def take_action(self, env):\n",
        "    while True:\n",
        "      # break if we make a legal move\n",
        "      move = input(\"Enter coordinates i,j for your next move (i,j=0..2): \")\n",
        "      i, j = move.split(',')\n",
        "      i = int(i)\n",
        "      j = int(j)\n",
        "      if env.is_empty(i, j):\n",
        "        env.board[i,j] = self.sym\n",
        "        break\n",
        "\n",
        "  # -----------------------------\n",
        "  def update(self, env):\n",
        "    pass\n",
        "\n",
        "  # -----------------------------\n",
        "  def update_state_history(self, s):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8v_TmuoVyBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# recursive function that will return all\n",
        "# possible states (as ints) and who the corresponding winner is for those states (if any)\n",
        "# (i, j) refers to the next cell on the board to permute (we need to try -1, 0, 1)\n",
        "# impossible games are ignored, i.e. 3x's and 3o's in a row simultaneously\n",
        "# since that will never happen in a real game\n",
        "def get_state_hash_and_winner(env, i=0, j=0):\n",
        "  results = []\n",
        "\n",
        "  for v in (0, env.x, env.o):\n",
        "    env.board[i,j] = v # if empty board it should already be 0\n",
        "    if j == 2:\n",
        "      # j goes back to 0, increase i, unless i = 2, then we are done\n",
        "      if i == 2:\n",
        "        # the board is full, collect results and return\n",
        "        state = env.get_state()\n",
        "        ended = env.game_over(force_recalculate=True)\n",
        "        winner = env.winner\n",
        "        results.append((state, winner, ended))\n",
        "      else:\n",
        "        results += get_state_hash_and_winner(env, i + 1, 0)\n",
        "    else:\n",
        "      # increment j, i stays the same\n",
        "      results += get_state_hash_and_winner(env, i, j + 1)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "def initialV_x(env, state_winner_triples):\n",
        "  # initialize state values as follows\n",
        "  # if x wins, V(s) = 1\n",
        "  # if x loses or draw, V(s) = 0\n",
        "  # otherwise, V(s) = 0.5\n",
        "  V = np.zeros(env.num_states)\n",
        "  for state, winner, ended in state_winner_triples:\n",
        "    if ended:\n",
        "      if winner == env.x:\n",
        "        v = 1\n",
        "      else:\n",
        "        v = 0\n",
        "    else:\n",
        "      v = 0.5\n",
        "    V[state] = v\n",
        "  return V\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "def initialV_o(env, state_winner_triples):\n",
        "  # this is (almost) the opposite of initial V for player x\n",
        "  # since everywhere where x wins (1), o loses (0)\n",
        "  # but a draw is still 0 for o\n",
        "  V = np.zeros(env.num_states)\n",
        "  for state, winner, ended in state_winner_triples:\n",
        "    if ended:\n",
        "      if winner == env.o:\n",
        "        v = 1\n",
        "      else:\n",
        "        v = 0\n",
        "    else:\n",
        "      v = 0.5\n",
        "    V[state] = v\n",
        "  return V\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "def play_game(p1, p2, env, draw=False):\n",
        "  # loops until the game is over\n",
        "  current_player = None\n",
        "  while not env.game_over():\n",
        "    # alternate between players\n",
        "    # p1 always starts first\n",
        "    if current_player == p1:\n",
        "      current_player = p2\n",
        "    else:\n",
        "      current_player = p1\n",
        "\n",
        "    # draw the board before the user who wants to see it makes a move\n",
        "    if draw:\n",
        "      if draw == 1 and current_player == p1:\n",
        "        env.draw_board()\n",
        "      if draw == 2 and current_player == p2:\n",
        "        env.draw_board()\n",
        "\n",
        "    # current player makes a move\n",
        "    current_player.take_action(env)\n",
        "\n",
        "    # update state histories\n",
        "    state = env.get_state()\n",
        "    p1.update_state_history(state)\n",
        "    p2.update_state_history(state)\n",
        "\n",
        "  if draw:\n",
        "    env.draw_board()\n",
        "\n",
        "  # do the value function update\n",
        "  p1.update(env)\n",
        "  p2.update(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqHtrWr1WVnA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  # train the agent\n",
        "  p1 = Agent()\n",
        "  p2 = Agent()\n",
        "\n",
        "  # set initial V for p1 and p2\n",
        "  env = Environment()\n",
        "  state_winner_triples = get_state_hash_and_winner(env)\n",
        "\n",
        "\n",
        "  Vx = initialV_x(env, state_winner_triples)\n",
        "  p1.setV(Vx)\n",
        "  Vo = initialV_o(env, state_winner_triples)\n",
        "  p2.setV(Vo)\n",
        "\n",
        "  # give each player their symbol\n",
        "  p1.set_symbol(env.x)\n",
        "  p2.set_symbol(env.o)\n",
        "\n",
        "  T = 10000\n",
        "  for t in range(T):\n",
        "    if t % 1000 == 0:\n",
        "      print(t)\n",
        "#       s = input()\n",
        "#     print(\"=========== Continue ============ \", t)\n",
        "#     s = input()\n",
        "    p1.set_verbose(False)\n",
        "    p2.set_verbose(False)\n",
        "    play_game(p1, p2, Environment())\n",
        "    \n",
        "\n",
        "  # play human vs. agent\n",
        "  # do you think the agent learned to play the game well?\n",
        "  human = Human()\n",
        "  human.set_symbol(env.o)\n",
        "  while True:\n",
        "    p1.set_verbose(True)\n",
        "    play_game(p1, human, Environment(), draw=2)\n",
        "    # I made the agent player 1 because I wanted to see if it would\n",
        "    # select the center as its starting move. If you want the agent\n",
        "    # to go second you can switch the human and AI.\n",
        "    answer = input(\"Play again? [Y/n]: \")\n",
        "    if answer and answer.lower()[0] == 'n':\n",
        "      break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJOtpsNQK6IN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}